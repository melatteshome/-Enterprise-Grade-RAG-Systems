{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage,  AIMessage\n",
    "\n",
    "\n",
    "\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/melat/tnx/week6/Enterprise-Grade-RAG-Systems/myenv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/home/melat/tnx/week6/Enterprise-Grade-RAG-Systems/myenv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure! String theory is a theoretical framework in physics that aims to provide a unified description of all fundamental particles and forces in the universe. It suggests that the fundamental building blocks of the universe are not point-like particles, but tiny, vibrating strings.\\n\\nHere are a few key points to understand about string theory:\\n\\n1. Dimensions: String theory requires the existence of extra dimensions beyond the three spatial dimensions (length, width, and height) we are familiar with. In fact, most versions of string theory propose that there are a total of ten dimensions, with six of them \"curled up\" into tiny, compact shapes.\\n\\n2. Vibrating Strings: In string theory, particles are not considered as separate entities, but rather as different modes of vibrations of these tiny strings. The different vibrational patterns of the strings correspond to different types of particles and their properties, such as mass and charge.\\n\\n3. Quantum Mechanics and General Relativity: String theory attempts to reconcile two major theories of physics: quantum mechanics, which describes the behavior of particles on microscopic scales, and general relativity, which explains the behavior of gravity on larger scales. By incorporating both theories, string theory provides a framework for understanding the behavior of particles and the nature of spacetime.\\n\\n4. String Landscape: One interesting aspect of string theory is its prediction of a vast \"landscape\" of possible solutions. Different configurations of the extra dimensions and vibrational patterns of strings can give rise to different physical properties, such as the masses and charges of particles. This landscape potentially allows for a wide range of possible universes, each with its own set of physical laws.\\n\\nIt\\'s important to note that string theory is still a highly active area of research, and many aspects of it are still being explored. While it holds great promise for a unified theory of physics, it has not yet been confirmed through experimental evidence. Nonetheless, it remains an intriguing and influential theory in theoretical physics.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_KEY,\n",
    "    model='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand string theory.\")\n",
    "]\n",
    "\n",
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "def create_qa_model():\n",
    "    # Load documents\n",
    "    loader = PyPDFLoader(\"./data/RAG.pdf\")\n",
    "    documents = loader.load()\n",
    "    # Split the documents into chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(len(texts))\n",
    "    # Select which embeddings we want to use\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    # Create the vector store to use as the index\n",
    "    db = Chroma.from_documents(texts, embeddings)\n",
    "    # Expose this index in a retriever interface\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "    return  texts, retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([Document(page_content=\"10 Academy Cohort A  \\nWeekly Challenge: Week 6  \\nPrecision RAG: Prompt Tuning For \\nBuilding Enterprise Grade RAG \\nSystems  \\n \\nBusiness objective    \\nPromptlyTech is an innovative e -business specializing in providing AI -driven solutions for \\noptimizing the use of Language Models (LLMs) in various industries. The company aims to \\nrevolutionize how businesses interact with LLMs, making the technology more a ccessible, \\nefficient, and effective. By addressing the challenges of prompt engineering, the company \\nplays a pivotal role in enhancing decision -making, operational efficiency, and customer \\nexperience across various industries. PromptlyTech's solutions are designed to cater to \\nthe evolving needs of a digitally -driven business landscape, where speed and accuracy \\nare key to staying competitive.  \\nThe company focuses on key services: Automatic Prompt Generation, Automatic \\nEvaluation Data Generation, and Prompt Te sting and Ranking.  \\n1. Automatic Prompt Generation Service:  \\n● This service streamlines the process of creating effective prompts, enabling \\nbusinesses to efficiently utilize LLMs for generating high -quality, relevant content. \\nIt significantly reduces the time and expertise required in crafting prompts \\nmanually.  \\n2. Aut omatic Evaluation Data Generation Service:\", metadata={'source': './data/RAG.pdf', 'page': 0}),\n",
       "  Document(page_content='● PromptlyTech’s service automates the generation of diverse test cases, ensuring \\ncomprehensive coverage and identifying potential issues. This enhances the \\nreliability and performance of LLM applications, saving si gnificant time in the \\nQA(Quality Assurance) process.  \\n3. Prompt Testing and Ranking Service:  \\n● PromptlyTech’s service evaluates and ranks different prompts based on \\neffectiveness, helping Users to get the desired outcome from LLM. It ensures that \\nchatbots and  virtual assistants provide accurate, contextually relevant responses, \\nthereby improving user engagement and satisfaction.  \\nBackground Context  \\n \\nIn the evolving field of artificial intelligence, Language Models (LLMs) like GPT -3.5 and \\nGPT -4 have become crucial for various applications. Their effectiveness, however, \\nheavily depends on the quality of the prompts they receive, leading to the emergence  of \\n\"prompt engineering\" as a key skill.  \\n \\nPrompt engineering is the craft of designing queries or statements to guide LLMs to \\nproduce desired outcomes. The challenge lies in the sensitivity of these models to \\nprompt nuances, where slight variations can yie ld vastly different results. This poses a \\nsignificant hurdle for users, especially in business contexts where accuracy and \\nrelevance are paramount.  \\n \\nThe need for simplified, efficient prompt engineering is clear. Automating and optimizing \\nthis process can save time, enhance LLM productivity, and make advanced AI \\ncapabilities more accessible to a broader range of users. The tasks of Automatic Prompt \\nGeneration, Evaluation Data Generation, and Prompt Testing and Ranking are aimed at \\naddressing these challenge s, streamlining the prompt engineering process for more \\neffective use of LLMs.  \\nLearning Outcomes  \\nSkills Development  \\n● Prompt Engineering Proficiency: Gain expertise in crafting effective prompts that \\nguide LLMs to desired outputs, understanding nuances and variations in \\nlanguage that impact model responses.  \\n● Critical Analysis: Develop the ability to critically analyze and evaluate the \\neffectiveness of different prompts based on their performance in varied \\nscenarios.  \\n● Technical Aptitude with LLMs: Enhance technical skills in using advanced \\nlanguage models like GPT -4 and GPT -3.5-Turbo, understanding their \\nfunctionalities and c apabilities.', metadata={'source': './data/RAG.pdf', 'page': 1}),\n",
       "  Document(page_content='● Problem -Solving and Creativity: Cultivate creative problem -solving skills by \\ngenerating innovative prompts and test cases, addressing complex and varied \\nobjectives.  \\n● Data Interpretation: Learn to interpret and analyze data from test cases and \\nprompt evaluations, deriving meaningful insights from performance metrics.  \\n \\nKnowledge Acquisition  \\n● Understanding of Language Models: Acquire a deeper understanding of how \\nLLMs function, including their strengths, limitations, and the principles behind \\ntheir responses.  \\n● Insights into Automated Evaluation Data Generation: Gain knowledge about the \\nmethodology and importance of creating test cases for evaluating prompt \\neffectiveness.  \\n● ELO Rating System and its Applications: Learn about the ELO rating system used \\nfor ranking prompts, understanding its mechanics and relevance in performance \\nevaluation.  \\n● Prompt Optimization Strategies: Understand various strategies for refining and \\noptimizing prompts to achieve better alignment with specific goals and desired \\noutcomes.  \\n● Industry Best Practices: Familiarize with the best practices in prompt engineering \\nwithin different industries, learning about real -world applications and challenges.  \\n \\nTeam  \\nTutors:  \\n- Yabebal  \\n- Emitinan  \\n- Rehmet  \\nBadges  \\nEach week, one user will be awarded one of the badges below for the best performance \\nin the category below.  \\n \\nIn addition to being the badge holder for that badge, each badge winner will get +20 points \\nto the overall score.  \\n \\nVisualization  - quality of visua lizations, understandability, skimmability, choice of \\nvisualization  \\nQuality of code  - reliability, maintainability, efficiency, commenting - in future this \\nwill be CICD /CML  \\nInnovative approach to analysis  -using latest algorithms, adding in research paper \\ncontent and other innovative approaches', metadata={'source': './data/RAG.pdf', 'page': 2}),\n",
       "  Document(page_content='Writing and presentation  - clarity of written outputs, clarity of slides, overall \\nproduction value  \\nMost supportive in the community  - helping others, adding links, tutoring those \\nstruggling  \\n \\nThe goal of this approach is to support and reward expertise in different parts of the \\nMachine learning engineering toolbox.  \\n \\nGroup Work Policy  \\nEveryone has to submit all their work individually.', metadata={'source': './data/RAG.pdf', 'page': 3}),\n",
       "  Document(page_content='Instruction: Automatic Prompt Engineering  \\nFundamental Tasks  \\nThe core tasks for this week’s challenge in Automatic Prompt Engineering are outlined \\nbelow:  \\n \\n1. Understand Prompt Engineering Tools and Concepts: Gain a thorough \\nunderstanding of the tools and theoretical concepts involved in prompt \\nengineering for Language Models (LLMs).  \\n \\n2. Familiarize with Language Models: Learn about the capabilities and functionali ties \\nof advanced LLMs like GPT -4 and GPT -3.5-Turbo.  \\n \\n3. Develop a Plan for Prompt Generation and Testing: Create a comprehensive plan \\nthat outlines the approach for automated prompt generation, test case creation, \\nand prompt evaluation.  \\n \\n4. Set Up a Development Environment: Prepare a suitable development \\nenvironment that supports the integration and testing of LLMs in the prompt \\nengineering process.  \\n \\n5. Design User Interface for Prompt System: Plan and initiate the development of a \\nuser -friendly interface for prompt  input, refinement, and performance analysis.  \\n \\n6. Plan Integration of LLMs: Strategize the integration of LLMs into the prompt \\nsystem for automated generation and testing.  \\n \\n7. Build and Refine Prompt Generation System: Develop the automated prompt \\ngeneration sys tem, ensuring it aligns with user inputs and objectives.  \\n \\n8. Develop Automatic Evaluation Data Generation System: Create a system for \\ngenerating test cases that evaluate the effectiveness of prompts in various \\nscenarios.  \\n \\n9. Implement Prompt Testing and Evaluati on Mechanism: Set up testing procedures \\nusing Monte Carlo matchmaking and ELO rating systems to evaluate and rank \\nprompts.  \\n \\n10. Refine and Optimize System Based on Feedback: Continuously refine the prompt \\ngeneration and evaluation system based on user feedback  and performance \\ndata.', metadata={'source': './data/RAG.pdf', 'page': 4}),\n",
       "  Document(page_content='Task 1: Review the Evolution of Automatic Prompt Engineering  \\nFocus on understanding the key developments in the field of automatic prompt \\nengineering for Language Models (LLMs).  \\n \\nStudy Key Concepts and Tools:  \\n● Understand the key components of an enterprise grade RAG systems  \\n○ Retrieval -augmented generation (RAG): What it is and why it’s a hot topic \\nfor enterprise AI  \\n○ Advanced RAG for LLMs/SLMs  \\n○ RAG for Text Generation Processes in Businesses  (check par t 1, 3, & 4 as \\nwell)  \\n○ Langchain Reterivers  \\n● Understand the need for advanced prompt engineering in building enterprise \\ngrade RAG systems  \\n○ Full Fine -Tuning, PEFT, Prompt Engineering, and RAG : Which One Is Right \\nfor You?  \\n○ Advanced Prompt Engineering - Practical Examples  \\n○ Prompt Engineering 201: Ad vanced methods and toolkits  \\n○ Do you agree with this article? RAG is Just Fancier Prompt Engineering  \\n● Understand the need for evaluating RAG components  \\n○ An Overview on RAG Evaluation  \\n○ Evaluating RAG: Using LLMs to Automa te Benchmarking of Retrieval \\nAugmented Generation Systems  \\n○ Evaluating RAG Applications with RAGAs  \\n○ RAG Evaluation Using LangChain and Ragas  \\n○ RAG System: Metrics and Evaluation Analysis with LlamaInde x \\n○ Evaluating RAG Part I: How to Evaluate Document Retrieval  \\n○ Evaluating RAG/LLMs in highly technical s ettings using synthetic QA \\ngeneration  \\n○ Evaluating Multi -Modal RAG  \\n● Understand the tools and techniques to automatically generate RAG evalu ation \\ndata  \\n○ The Tech Buffet #16: Quickly Evaluate your RAG Without Manually \\nLabeling Test Data  \\n○ Generating a Synthetic Dataset for RAG  \\n○  \\n● Learn key packages to planning, building, testing, monitoring, and deploying \\nenterprise grade RAG system  \\n○ Iterate on LLMs faster: Measure LLM quality and catch regressions  \\n○ Building RAG -based LLM Applications for Production  \\n○ ARES: An Automated Evaluation Framework for Retrieval -Augmented \\nGeneration Systems  \\n● Understand the end -to-end technology stack of RAG systems', metadata={'source': './data/RAG.pdf', 'page': 5}),\n",
       "  Document(page_content='○ End-to-End LLMOps Platfo rm \\n○ An Enterprise -Grade Reference Architecture for the Production \\nDeployment of LLMs Using the RAG Patter n on Azure OpenAI  \\nTask 2: Design and Develop the Prompt Generation System  \\n● Users can input a description of their objective or task and specify a few \\nscenarios along with their expected outputs.  \\n● Write or adopt sophisticated algorithms, you generate multipl e prompt options \\nbased on the provided information.  \\n● This automated prompt generation process saves time and provides a diverse \\nrange of alternatives to consider. But add an evaluation metrics that check \\nwhether the generated prompt candidate aligns with t he input description.  \\nTask 3: Implement Evaluation Data Generation and Evaluation  \\nTo further enhance the prompt generation process, incorporate automatic Evaluation \\nData Generation.  \\n● By analysing the description provided by the user,  create a set of test cases that \\nserve as evaluation benchmarks for the prompt candidates.  \\n● These test cases simulate various scenarios, enabling users to observe how each \\nprompt performs in different contexts.  \\n● The generated test cases serve as a starting point, sparking creati vity and \\ninspiring additional test cases for comprehensive evaluation.  \\nTask 4 : Prompt Testing and Ranking  \\nGoals  \\nComprehensive Evaluation: Provide a robust system that uses various methodologies \\nfor a thorough assessment of prompts.  \\nCustomizable and User -Centric:  Allow users to choose or customize their preferred \\nevaluation methods.  \\nDynamic and Adaptive: Ensure the system remains flexible and adaptive, capable of \\nincorporating new ranking methodologies as they emerge.  \\n \\nPrimary Methods  \\n \\n● Monte Carlo Matchmaki ng: This method is used to select and match different \\nprompt candidates against each other. The Monte Carlo method, known for its \\napplications in problem -solving and decision -making processes, helps in \\noptimizing the information gained from each prompt bat tle. By simulating various \\nmatchups, it allows the system to test the effectiveness of each prompt in \\ndifferent scenarios.  \\n● ELO Rating System:  This system, which is commonly used in chess and other \\ncompetitive games, rates the prompts based on their perfor mance in the battles. \\nEach prompt candidate is assigned a rating that reflects its success in previous', metadata={'source': './data/RAG.pdf', 'page': 6}),\n",
       "  Document(page_content=\"matchups. The system takes into account not just the number of wins but also the \\nstrength of the opponents each prompt has defeated. This rating helps in  \\nobjectively ranking the prompts based on their effectiveness.  \\n \\nAdditional Ranking and Matching Mechanisms  \\n● TrueSkill Rating System : Ideal for scenarios involving multiple competitors, \\nadjusting ratings based on not just wins and losses but also the uncertainty in \\nperformance.  \\n● Glicko Rating System:  Similar to ELO but with added flexibility, accounting for \\nthe volatility in a player's (or prompt’s) performance and the reliability of their \\nrating.  \\n● Bayesian Rating Systems : Applies Bayesian inference for a probabilistic \\napproach to rating, considering u ncertainties and contextual variations in prompt \\nperformance.  \\n● Pairwise Comparison Methods : Involves direct comparisons between pairs of \\nprompts, potentially integrating user preferences or expert evaluations into the \\nranking process.  \\n● Categorical Ranking : Instead of a numerical rating, prompts are categorized \\nbased on performance criteria like creativity, relevance, etc., for more qualitative \\nassessments.  \\n● Adaptive Ranking Algorithms : Algorithms that learn and adjust over time, \\nconsidering historical performa nce data and evolving user preferences or \\nrequirements.  \\n● Semantic Similarity Matching : Using NLP techniques to match prompts based \\non semantic content, ideal for understanding nuanced differences in prompt \\neffectiveness.  \\n \\nYou should adopt an innovative appr oach to prompt evaluation by utilizing Monte Carlo \\nmatchmaking  and  ELO rating systems,  or any alternative method to match and rank.  \\nTask 5: User Interface Development  \\nDevelop a user -friendly interface for interacting with the prompt engineering system.  \\n● UI Design: Plan and design a user interface that allows users to easily input data, \\nreceive prompts, and view evaluation results.  \\n● UI Implementation: Develop and integrate the user interface with the backend \\nprompt engineering system.  \\nTask 6: System Integrati on and Testing  \\n● Integrate all components of the system and conduct comprehensive testing.  \\n● Integrate the prompt generation, Evaluation Data Generation, evaluation, and user \\ninterface components.  \\n● Test the entire system for functionality, usability, and perfor mance. Refine based \\non feedback and test results.\", metadata={'source': './data/RAG.pdf', 'page': 7}),\n",
       "  Document(page_content='Tutorials Schedule  \\nIn the following, the colour purple  indicates morning sessions, and blue  indicates \\nafternoon sessions.  \\nMonday: Understanding Prompt engineering  \\nHere the trainees will understand the w eek’s challenge.  \\n● Introduction to Week Challenge (Yabebal)  \\n● Introduction and challenge to prompt engineering (Fikerte)  \\n \\nKey Performance Indicators:  \\n \\n● Understanding week’s challenge  \\n● Understanding the prompt engineering  \\n● Ability to reuse previous knowledge  \\nTuesday  \\n● RAG components (Rehmet)  \\n● Techniques to improving R (Retrievers) in RAG (Emitnan)  \\n \\nKey Performance Indicators:  \\n \\n● Understanding Prompt ranking  \\n● Understanding prompt matching  \\n● Ability to reuse previous knowledge  \\n \\nWednesday  \\n● RAG Evaluation Data Generation  (Abel)  \\n● Understanding of prompt matching and ranking (Mahlet)  \\nThursday  \\n● RAG evaluation metrics (Emitnan)  \\n● RAGObs - DevObs of RAG development and production deployment', metadata={'source': './data/RAG.pdf', 'page': 9}),\n",
       "  Document(page_content=\"Deliverables  \\nNOTE: Document should be a PDF stored in google drive or published blog link. DO NOT \\nSUBMIT A LINK as PDF! If you want to submit pdf document, it should be the content of \\nyour report not a link.  \\nInterim Submission - Wednesday 8pm UTC  \\n● Link to your code in GitHub  \\n○ Repository where you will be using to complete the tasks in this week's \\nchallenge. A minimum requirement is that you have a well structured \\nrepository and some coding progress is made.  \\n \\n \\n● A review report of your reading and understanding of Task 1 and any progress you \\nmade in other tasks.  \\nFeedback  \\nYou may not receive detailed comments on your interim submission, but will receive a \\ngrade.  \\nFinal Submission - Saturday 8pm UTC  \\n● Link to your code in GitHub  \\n○ Complete work  for Automatic prompt generation  \\n○ Complete  work  for Automatic evaluation  \\n○ Complete work for Evaluation Data Generation  \\n \\n● A blog post entry (which you can submit for example to Medium publishing) or a \\npdf report. .  \\nFeedback  \\nYou will receive comments/feedback in addition to a grade.\", metadata={'source': './data/RAG.pdf', 'page': 10}),\n",
       "  Document(page_content='References  \\n● Meistrari didn’t see a good solution for prompt engineering, so it’s building one  \\n● AutoPrompt: Eliciting  Knowledge from Language Models with Automatically \\nGenerated Prompts  \\n● Large Language Models Are Human -Level Prompt Engineers  \\n● Prompt Engineering  \\n● How to Create a Monte Carlo Simulation using Python  \\n● Monte Carlo Method Explained  \\n● What is Monte Carlo Simulation? How does it work?  \\n● Elo Rating Algorithm  \\n● Elo algorithm implementation in Python  \\n● TrueSkillTM: A Bayesian skill rating system  \\n \\nCompanies doing something similar to this project  \\n● AI Prompt Generator (promptlygenerated.com)', metadata={'source': './data/RAG.pdf', 'page': 11})],\n",
       " VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7fef129256f0>, search_kwargs={'k': 2}))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_qa_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
